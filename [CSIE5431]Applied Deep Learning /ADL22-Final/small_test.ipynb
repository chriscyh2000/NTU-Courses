{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b09901073/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at google/mt5-small were not used when initializing MT5Model: ['lm_head.weight']\n",
      "- This IS expected if you are initializing MT5Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MT5Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import EncoderDecoderModel\n",
    "\n",
    "from encoder import BaseEncoder\n",
    "from decoder import BaseSeqDecoder\n",
    "\n",
    "encoder = BaseEncoder()\n",
    "decoder = BaseSeqDecoder(1000)\n",
    "\n",
    "model = EncoderDecoderModel(encoder=encoder, decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dummy_text.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m## load texts from dummy_text.txt\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mdummy_text.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m      3\u001b[0m     texts \u001b[39m=\u001b[39m [line\u001b[39m.\u001b[39mstrip() \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m f\u001b[39m.\u001b[39mreadlines()]\n\u001b[1;32m      5\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mdummy_label.txt\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dummy_text.txt'"
     ]
    }
   ],
   "source": [
    "## load texts from dummy_text.txt\n",
    "with open(\"dummy_text.txt\", \"r\") as f:\n",
    "    texts = [line.strip() for line in f.readlines()]\n",
    "\n",
    "with open(\"dummy_label.txt\", \"r\") as f:\n",
    "    labels = [[int(s) for s in line.strip().split(\", \")] for line in f.readlines()]\n",
    "\n",
    "print(texts)\n",
    "print(labels)\n",
    "\n",
    "data = [{\"text\": text, \"labels\": label} for text, label in zip(texts, labels)]\n",
    "import json\n",
    "with open(\"dummy_train.json\", \"w\") as f:\n",
    "    json.dump(data, f, indent=4)\n",
    "\n",
    "import datasets\n",
    "dataset = datasets.load_dataset(\"json\", data_files=\"dummy_train.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[574], [318, 573, 625, 628, 564, 304, 468, 522, 580]]\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b09901073/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "dataset = datasets.load_dataset(\"squad_v2\", split=\"validation[:10%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "trainer = transformers.Seq2SeqTrainer(\n",
    "    model=model,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MT5Config {\n",
      "  \"d_ff\": 1024,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"mt5\",\n",
      "  \"num_decoder_layers\": 8,\n",
      "  \"num_heads\": 6,\n",
      "  \"num_layers\": 8,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"T5Tokenizer\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": \"google/mt5-small\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import MT5Config\n",
    "config_d = MT5Config(model_name)\n",
    "print(config_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized configuration class <class 'transformers.models.mt5.configuration_mt5.MT5Config'> for this kind of AutoModel: AutoModelForCausalLM.\nModel type should be one of BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CamembertConfig, CodeGenConfig, CTRLConfig, Data2VecTextConfig, ElectraConfig, ErnieConfig, GPT2Config, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, MarianConfig, MBartConfig, MegatronBertConfig, MvpConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PLBartConfig, ProphetNetConfig, QDQBertConfig, ReformerConfig, RemBertConfig, RobertaConfig, RoCBertConfig, RoFormerConfig, Speech2Text2Config, TransfoXLConfig, TrOCRConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m EncoderDecoderConfig, EncoderDecoderModel\n\u001b[1;32m      2\u001b[0m config \u001b[39m=\u001b[39m EncoderDecoderConfig\u001b[39m.\u001b[39mfrom_encoder_decoder_configs(config_e, config_d)\n\u001b[0;32m----> 4\u001b[0m model \u001b[39m=\u001b[39m EncoderDecoderModel(config\u001b[39m=\u001b[39;49mconfig)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:215\u001b[0m, in \u001b[0;36mEncoderDecoderModel.__init__\u001b[0;34m(self, config, encoder, decoder)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[39mif\u001b[39;00m decoder \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mauto\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodeling_auto\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoModelForCausalLM\n\u001b[0;32m--> 215\u001b[0m     decoder \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_config(config\u001b[39m.\u001b[39;49mdecoder)\n\u001b[1;32m    217\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder \u001b[39m=\u001b[39m encoder\n\u001b[1;32m    218\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder \u001b[39m=\u001b[39m decoder\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:412\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_config\u001b[0;34m(cls, config, **kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[1;32m    410\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39m_from_config(config, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 412\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    413\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    414\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    415\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Unrecognized configuration class <class 'transformers.models.mt5.configuration_mt5.MT5Config'> for this kind of AutoModel: AutoModelForCausalLM.\nModel type should be one of BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CamembertConfig, CodeGenConfig, CTRLConfig, Data2VecTextConfig, ElectraConfig, ErnieConfig, GPT2Config, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, MarianConfig, MBartConfig, MegatronBertConfig, MvpConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PLBartConfig, ProphetNetConfig, QDQBertConfig, ReformerConfig, RemBertConfig, RobertaConfig, RoCBertConfig, RoFormerConfig, Speech2Text2Config, TransfoXLConfig, TrOCRConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig."
     ]
    }
   ],
   "source": [
    "from transformers import EncoderDecoderConfig, EncoderDecoderModel\n",
    "config = EncoderDecoderConfig.from_encoder_decoder_configs(encoder=)\n",
    "\n",
    "model = EncoderDecoderModel(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseSeqDecoder(\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(1000, 512)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 6)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "decoder = BaseSeqDecoder(1000)\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5bdecbfffec014002166796a\n",
      "5bfd47782d018e0020e4b0e45fc4a352d375951a03cc0d456090dda489d06d6a564c9a785fd9b1ce0fb8aa8b32928d5b6030c9cd99e14cc2401e66b95cad79b0d133060020f44c5e5f1f9092673f8866bf9604af5f59ff38f24eac1485f8097f607015a2db27b36747727e80\n",
      "[318, 573, 625, 628, 564, 304, 468, 522, 580]\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "import pandas as pd\n",
    "\n",
    "tokenizer = Tokenizer.from_file(\"course_tokenizer.json\")\n",
    "\n",
    "with open(\"train.csv\", \"r\") as f:\n",
    "    df = pd.read_csv(f)\n",
    "    users = df[\"user_id\"]\n",
    "    courses = [\"\".join(course.split(\" \")) for course in df[\"course_id\"]]\n",
    "\n",
    "course_tokens = [t.ids for t in tokenizer.encode_batch(courses)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[574]\n",
      "[318, 573, 625, 628, 564, 304, 468, 522, 580]\n"
     ]
    }
   ],
   "source": [
    "print(course_tokens[0])\n",
    "print(course_tokens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/mt5-small were not used when initializing MT5Model: ['lm_head.weight']\n",
      "- This IS expected if you are initializing MT5Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MT5Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/b09901073/.local/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Using custom data configuration default-37da52e0b936cc24\n",
      "Found cached dataset json (/home/b09901073/.cache/huggingface/datasets/json/default-37da52e0b936cc24/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
      "100%|██████████| 1/1 [00:00<00:00, 376.17it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 193.22ba/s]\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text. If text are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "/home/b09901073/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/b09901073/anaconda3/envs/adl/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:32: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "***** Running training *****\n",
      "  Num examples = 2\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1\n",
      "  Number of trainable parameters = 10466496\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/b09901073/.local/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/home/b09901073/anaconda3/envs/adl/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import EncoderDecoderModel, TrainingArguments, AutoTokenizer, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "import transformers\n",
    "import datasets\n",
    "\n",
    "from encoder import BaseEncoder\n",
    "from decoder import BaseSeqDecoder\n",
    "\n",
    "import pandas as pd\n",
    "### mode = [\"seq\", \"val\"]\n",
    "mode = \"seq\"\n",
    "data_path = \"dummy_train.json\"\n",
    "model_name = \"google/mt5-small\"\n",
    "\n",
    "if mode == \"seq\":\n",
    "    encoder = BaseEncoder(model_name=model_name)\n",
    "    decoder = BaseSeqDecoder(1000)\n",
    "    #config = EncoderDecoderModel.from_encoder_decoder_pretrained(encoder, decoder)\n",
    "    #model = EncoderDecoderModel(config=config)\n",
    "    model = EncoderDecoderModel(encoder=encoder, decoder=decoder)\n",
    "    model.config.decoder_start_token_id = 0\n",
    "    model.config.pad_token_id = 3\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    dataset = datasets.load_dataset(\"json\", data_files=\"dummy_train.json\")\n",
    "\n",
    "    model.train()\n",
    "    model.to(\"cuda\")\n",
    "\n",
    "    # \"text\", \"label\"\n",
    "    def collate_fn(examples):\n",
    "        tokens = tokenizer(examples[\"text\"], truncation=True, max_length=512)\n",
    "        examples[\"input_ids\"] = tokens[\"input_ids\"]\n",
    "        examples[\"attention_mask\"] = tokens[\"attention_mask\"]\n",
    "        return examples\n",
    "\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=\"longest\", max_length=512)\n",
    "\n",
    "    dataset = dataset.map(collate_fn, batched=True)\n",
    "    dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=10,\n",
    "        save_steps=1000,\n",
    "        save_total_limit=2,\n",
    "        learning_rate=1e-4,\n",
    "        fp16=True,\n",
    "    )\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/mt5-small were not used when initializing MT5Model: ['lm_head.weight']\n",
      "- This IS expected if you are initializing MT5Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MT5Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([728, 512, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b09901073/adlfinal/adl-final/utils.py:26: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1666642991888/work/torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  return torch.tensor(dataset)\n"
     ]
    }
   ],
   "source": [
    "from models import Seq2SeqWithAttentionModel\n",
    "from encoder import BaseEncoder\n",
    "from decoder import BaseSeqDecoder\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "encoder = BaseEncoder()\n",
    "decoder = BaseSeqDecoder(1000)\n",
    "course_embed_path = \"processed_datas/train_courses_embeddings.pkl\"\n",
    "model = Seq2SeqWithAttentionModel(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    course_embed_path=course_embed_path,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b09901073/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "data_path = \"processed_datas/train_users.json\"\n",
    "embed_path = \"processed_datas/train_users_embeddings.pkl\"\n",
    "\n",
    "from utils import get_dataset\n",
    "dataset = get_dataset(data_path, embed_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=\"longest\", max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b09901073/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading: 100%|██████████| 1.18k/1.18k [00:00<00:00, 612kB/s]\n",
      "Downloading: 100%|██████████| 4.10k/4.10k [00:00<00:00, 1.56MB/s]\n",
      "Downloading: 100%|██████████| 856/856 [00:00<00:00, 482kB/s]\n",
      "Downloading: 100%|██████████| 546/546 [00:00<00:00, 300kB/s]\n",
      "Downloading: 100%|██████████| 409M/409M [00:19<00:00, 20.7MB/s]   \n",
      "Downloading: 100%|██████████| 112/112 [00:00<00:00, 46.1kB/s]\n",
      "Downloading: 100%|██████████| 319/319 [00:00<00:00, 168kB/s]\n",
      "Downloading: 100%|██████████| 110k/110k [00:00<00:00, 181kB/s]  \n",
      "No sentence-transformers model found with name /home/b09901073/.cache/torch/sentence_transformers/shibing624_text2vec-base-chinese. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embeddings:\n",
      "[[-4.4353193e-04 -2.9734758e-01  8.5790151e-01 ... -5.2770156e-01\n",
      "  -1.4315669e-01 -1.0007857e-01]\n",
      " [ 6.5362054e-01 -7.6667659e-02  9.5962334e-01 ... -6.0122514e-01\n",
      "  -1.6795505e-03  2.1457668e-01]]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "m = SentenceTransformer(\"shibing624/text2vec-base-chinese\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embeddings:\n",
      "[[ 0.5227772   0.24932477  1.5166957  ... -0.5019281  -0.15973377\n",
      "  -0.05946869]\n",
      " [ 0.22392651 -0.17477436  1.202549   ...  0.08209081 -0.67742175\n",
      "   0.5043854 ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4592028260231018"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = ['[OCCUPATION]', '[OCCUPATION]藝文設計,製造業,服務業']\n",
    "\n",
    "sentence_embeddings = m.encode(sentences)\n",
    "print(\"Sentence embeddings:\")\n",
    "print(sentence_embeddings)\n",
    "\n",
    "## cosine similarity\n",
    "from scipy.spatial.distance import cosine\n",
    "cosine(sentence_embeddings[0], sentence_embeddings[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 654, 660, 22, 668, 669, 671, 677, 680, 41, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 439, 694, 695, 696, 697, 698, 699, 62, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 123}\n",
      "{0, 1, 2, 3, 4, 521, 12, 17, 20, 21, 22, 533, 536, 34, 41, 43, 49, 62, 72, 586, 75, 87, 97, 98, 99, 101, 107, 108, 641, 650, 141, 144, 153, 677, 166, 681, 689, 697, 699, 705, 708, 712, 713, 202, 714, 715, 205, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 228, 233, 239, 240, 247, 254, 269, 272, 274, 276, 278, 289, 307, 309, 316, 324, 331, 338, 340, 351, 353, 354, 393, 408, 414, 439, 478, 488, 502}\n",
      "{0, 1, 2, 3, 4, 22, 677, 41, 681, 689, 439, 697, 699, 62, 705, 708, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "train_json = json.load(open(\"processed_datas/train_users.json\"))\n",
    "valid_json = json.load(open(\"processed_datas/val_unseen.json\"))\n",
    "\n",
    "\n",
    "train_labels = set()\n",
    "for user in train_json:\n",
    "    train_labels.update(user[\"labels\"])\n",
    "\n",
    "valid_labels = set()\n",
    "for user in valid_json:\n",
    "    valid_labels.update(user[\"labels\"])\n",
    "\n",
    "print(set([i for i in range(732)]) - train_labels)\n",
    "print(set([i for i in range(732)]) - valid_labels)\n",
    "print(set([i for i in range(732)]) - valid_labels - train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9480 1.8213142156600952\n",
      "8757 0.38095238095238093\n",
      "31016 0.36363636363636365\n",
      "14844 0.3620689655172414\n",
      "18922 0.36065573770491804\n",
      "30848 0.35185185185185186\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "train_embed = pickle.load(open(\"processed_datas/train_user_embeds32.pkl\", \"rb\"))\n",
    "valid_embed = pickle.load(open(\"processed_datas/val_unsee_embeds32.pkl\", \"rb\"))\n",
    "\n",
    "raw_train_embed = pickle.load(open(\"processed_datas/train_user_embeds.pkl\", \"rb\"))\n",
    "raw_valid_embed = pickle.load(open(\"processed_datas/val_unsee_embeds.pkl\", \"rb\"))\n",
    "\n",
    "task = valid_embed[30]\n",
    "\n",
    "### find the most similar user in train set\n",
    "### use euclidean distance\n",
    "from scipy.spatial.distance import euclidean\n",
    "import numpy as np\n",
    "min_idx = 0\n",
    "min_dist = euclidean(train_embed[0], task)\n",
    "for i, user in enumerate(train_embed):\n",
    "    dist = euclidean(user, task)\n",
    "    if dist < min_dist:\n",
    "        min_dist = dist\n",
    "        min_idx = i\n",
    "    \n",
    "print(min_idx, min_dist)\n",
    "\n",
    "### find the most similar user in train set\n",
    "\n",
    "task = raw_valid_embed[2000]\n",
    "raw_train_embed = np.array(raw_train_embed)\n",
    "\n",
    "### find the most similar user in train set\n",
    "### use jaccard distance\n",
    "import numpy as np\n",
    "min_idx = 0\n",
    "dist = np.minimum(raw_train_embed, task).sum(axis=1) / np.maximum(raw_train_embed, task).sum(axis=1)\n",
    "\n",
    "k = 5\n",
    "topk = np.argsort(-dist)[:k]\n",
    "for i in topk:\n",
    "    print(i, dist[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 (default, Nov 24 2022, 15:19:38) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "825ef558ec91ccb369c5ade66cb9ed2dc49eb7a1baf3f301fee22ad84418e33e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0196e7b2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Numerical Integration II\n",
    "\n",
    "\n",
    "\n",
    "[//]: # \"(Robert Casella 2004 Springer) Monte Carlo Statistical Methods\"\n",
    "\n",
    "[//]: # \"_**Importance sampling; MCMC (Gibbs, Metropolis-H); GHK**_\"\n",
    "\n",
    "[//]: # \"Here we introduce the method of Monte Carlo Integrations.\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Monte Carlo Integration\n",
    "\n",
    "\n",
    "> <div class=\"alert alert-block alert-info\">\n",
    "    In this Jupyter notebook document I labeled and cross-referenced equations using something like <b>\\label{eq:myeq1}</b> and <b>\\refeq{eq:myeq1}</b>. It may not work correctly if you use VSCode to read the document. Try opening and reading this .ipynb document in web browsers (as it was intended).\n",
    "</div>\n",
    "\n",
    "\n",
    "We often start a gentle introduction of a new method using a simple example such as $I = \\int_a^b g(x) dx$, where $I$ is the area under $g(x)$ in $[a,b]$. Here, when we are introducing the (Quasi) Monte Carlo method, we will start from a slightly more general (and thus a slightly complicated) problem. The general setup allows us to see the whole picture in a more straightforward way. As you will see, the simple problem above would be a special case of the general problem.\n",
    "\n",
    "\n",
    "### a general setup\n",
    "\n",
    "\n",
    "Let's consider an integration problem in the following form:\n",
    "\n",
    "\\begin{equation}\n",
    "I = \\int_\\Omega f(x) p(x) dx,\\label{eq:main1}\n",
    "\\end{equation}\n",
    "\n",
    "where \n",
    "\n",
    "- $p(x)$ is a nonnegative function which is often referred to as the **weight function**,\n",
    "\n",
    "- $\\Omega$ is the domain of the integral; for example,\n",
    "  - $\\Omega = [a,b]$ where both $a$ and $b$ are finites (proper integral);\n",
    "  - $\\Omega = [-\\infty, b], [a, \\infty], [-\\infty, \\infty]$, etc. (improper integral).\n",
    "\n",
    "\n",
    "\n",
    "> **You:** What? A *weight function*? How could this be a *general* setup? It's not general at all. My integration problem does not have a weight function.\n",
    ">    \n",
    "> **Me:** Well, p(x)=1 is also a \"nonnegative function\" and thus a \"weight function\". We will see more later.\n",
    "\n",
    "\n",
    "###### weight functions taking the form of exponential functions\n",
    "\n",
    "The weight function can take various forms which correspond to different methods of numerical integration. For instance, if $p(x)$ is an exponential function in specific forms, we could apply the quadrature rules (we've seen this before). Two examples:\n",
    "\n",
    "- If $p(x) = e^{-x^2}$ and $\\{a, b\\} = \\{-\\infty, \\infty\\}$, then we could apply the Gauss-Hermite quadrature rule. \n",
    "\n",
    "- If $p(x)= e^0 = 1$ and $\\{a, b\\} = \\{-1, 1\\}$, $I$ is the area under the $f(x)$ function between $-1$ and $1$ and can be approximated using the Gauss-Legendre rule.\n",
    "\n",
    "\n",
    "###### weight functions taking the form of probability density functions\n",
    "\n",
    "Here, we consider another possibility where $p(x)$ is a **probability density function** of a continuous random variable $X$ with the domain equal to $\\Omega$; that is,\n",
    "\n",
    "\\begin{aligned}\n",
    " \\int_\\Omega p(x) dx = 1.\n",
    "\\end{aligned}. \n",
    "\n",
    "\n",
    "In this case, the integration problem in \\eqref{eq:main1} can be interpreted as the expected value of $f(x)$ where the $x$ is drawn from $\\Omega$ with the probability dictated by $p(x)$. That is,\n",
    "\n",
    "\\begin{aligned}\n",
    "I = \\int_\\Omega f(x) p(x) dx  =\\ E_p[f(x)].\n",
    "\\end{aligned}\n",
    "\n",
    "Note that we use the subscript $p$ in the notation $E_p[\\cdot]$ to indicate that the expectation of $f(x)$ is calculated from $x$ which is drawn based on the density function of $p(x)$. \n",
    "\n",
    "> So, the notation $E_p[f(x)]$ conveys two important information: the objective function (i.e., $f(x)$) to which we wish to calculate the expected value, and the way $x$ is generated (i.e., $p(x)$) in calculating the expected value.\n",
    "\n",
    "> The _expectation_ interpretation is easy to see in the case of discrete random variables, such as $E_\\pi[f(x)] = f(x_1)\\pi_1 +  f(x_2)\\pi_2 + \\ldots + f(x_n)\\pi_n$, $\\sum_{j=1}^n \\pi_j = 1$. If the variable is continuous, the analogous notation is \n",
    "> $E_p[f(x)] = \\int_\\Omega f(x) p(x) dx$, where $\\int_\\Omega p(x)dx = 1$.\n",
    "\n",
    "\n",
    "###### approximation using the strong law of large numbers\n",
    "\n",
    "Why we bother to express an integration problem as an expected value? Because the expected value could be approximated using the sample average (empirical average) by the strong law of large numbers:\n",
    "\n",
    "\\begin{align}\n",
    "I = E_p[f(x)] \\approx \\frac{1}{n} \\sum_{j=1}^n f(x_j), \\label{eq:lln}\n",
    "\\end{align}\n",
    "\n",
    "where the sample $(x_1, x_2, \\ldots, x_n)$ is generated according to the density function $p(x)$. In other words, the value of the integration is calculated by drawing a sample of $x_j$s according to $p(x)$, substitute them into $f(x)$ and compute the sample average of $f(x)$.\n",
    "\n",
    "Note that the sentence \"_where the sample ..._\" is an important part of the problem statement. You cannot skip it in most cases.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### Remarks:\n",
    "\n",
    "- How do we draw the sample of $(x_1, x_2, \\ldots, x_n)$ from $p(x)$ in practice?\n",
    "\n",
    "\n",
    "  - We could draw the sample randomly from $p(x)$ using the various sampling methods we introduced earlier.\n",
    "\n",
    "    - If $p(x)$ is well defined and the inverse function is available, we could use the inverse sampling method.\n",
    "    \n",
    "    - If the inverse function of $p(x)$ is unavailable, we could use the reject-accept method (which produces independent samples but the sampling process could be inefficient and impractical) or various MCMC methods including Gibbs sampling or Metropolis-Hasting sampling (which is efficient but the samples are not independent; asymptotically these samples have the desired distribution). \n",
    " \n",
    "  - We could take a more strategic sample of $(x_1, x_2, \\ldots, x_n)$ using the low discrepancy sequence, which we will introduce later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3341f8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### a special case of $x \\sim U(0,1)$\n",
    "\n",
    "\n",
    "In the case of $I = \\int_0^1 f(x) p(x) dx$ where $x$ follows a uniform distribution in $[0,1]$, i.e., $x \\sim U(0,1)$, we have $p(x) = \\frac{1}{1-0} = 1$ which is trivial. Thus, we often skip the subscript in the expectation notation for such a case and so the problem becomes \n",
    "\n",
    "\\begin{align}\n",
    "I = \\int_0^1 f(x) p(x) dx = \\int_0^1 f(x) dx & =  E[f(x)] \\notag \\\\\n",
    "      & \\approx \\frac{1}{n}\\sum_{j=1}^n f(x_j), \\label{eq:lln_uni}\n",
    "\\end{align}\n",
    "\n",
    "where the sample $(x_1, x_2, \\ldots, x_n)$ is drawn from $U(0,1)$. Therefore, $E[f(x)]$, which is without the subscript, is generally understood as the expectation of $f(x)$ where $x$ is drawn from $U(0,1)$.\n",
    "\n",
    "Note that \\eqref{eq:lln} and \\eqref{eq:lln_uni} may look the same. However, the former's $x_i$s sample is drawn based on the density of $p(x)$ while the latter's is drawn from $U(0,1)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c058af3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### a typical integration problem (and a special case of the general setup)\n",
    "\n",
    "Now we are ready to talk about a more typical integration problem, which may look simpler and could be treated as a special case of the general problem:\n",
    "\n",
    "\\begin{aligned}\n",
    " I = \\int_\\Omega f(x) dx.\n",
    "\\end{aligned}\n",
    "\n",
    "\n",
    "In this type of problems, there is no $p(x)$ (the weight function) to begin with (or put differently, $p(x)=1$) and the integration's domain is not necessarily $[0,1]$. It is actually a more common problem you may encounter. For example, $I= \\int_a^b f(x) dx$ which is the area under the curve of $f(x)$ in $[a,b]$. \n",
    "> <div class=\"alert alert-block alert-success\">\n",
    "<b>Question:</b> If there is no probability density function in the integrand, could we still express the integration as an expected value of f(x)?<br>\n",
    "<b>Answer:</b> Yes! We simply assign one to it (and make necessary adjustments)!\n",
    "</div> \n",
    "\n",
    "#### finite domain (proper integral)\n",
    "\n",
    "If the domain of the integration is finite, e.g., $\\Omega = [a,b]$ where $a$ and $b$ are finites, we could simply assign a uniform distribution in $[a,b]$ to it: $p(x) = 1/(b-a)$. The uniform probability is handy because the pdf does not depend on $x$. For instance,\n",
    "\n",
    "\\begin{aligned}\n",
    "I  = \\int_a^b f(x) dx & = \\int_a^b  \\frac{f(x)}{p(x)} p(x) dx \\\\\n",
    "  & = (b-a) \\int_a^b f(x) p(x) dx = (b-a) E_U[f(x)],\n",
    "\\end{aligned}\n",
    "\n",
    "where $x \\sim U(a,b)$. In this example, we are able to express $I$ as an expected value of $f(x)$ over $x \\sim U(a,b)$ multiplied by the \"_**volume**_\" $(b-a)$. It is an effective approach, because we can then take the advantage of the law of large numbers and use the approximation:\n",
    "\n",
    "\\begin{aligned}\n",
    "I = \\int_a^b f(x) dx = (b-a) E_U[f(x)] \\approx (b-a)\\left[ \\frac{1}{n}\\sum_{j=1}^n f(x_j) \\right],\n",
    "\\end{aligned}\n",
    "\n",
    "where the sample $(x_1, x_2, \\ldots, x_n)$ is drawn from $U(a,b)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5327bd",
   "metadata": {},
   "source": [
    "#### not finite domain (improper integral)\n",
    "\n",
    "What if the domain is not finite, such as $[a, \\infty]$, $[-\\infty,b]$, $[-\\infty, \\infty]$, etc.? The above method may not seem to work because there is no uniform distribution with unbounded domains. It turns out that we can circumvent the problem by transforming the infinite domain of $x$ to a finite domain of $t$ using the change of variables. \n",
    "\n",
    "Just to give you a hint: Suppose the domain is $x \\in [a, \\infty]$ where $a$ is finite. Let's consider the transformation rule $x = a + t/(1-t)$. If $x=a$, it's easy to see that $t=0$ is a solution. If $x\\rightarrow \\infty$, we see that $t=1$ would fit the rule. Therefore, using the rule, we transform $x \\in [a, \\infty]$ to $t \\in [0,1]$ which is finite. Provided that the entire integration problem is properly transformed from $x$ to $t$, we could apply the above method again (\"assign a probability function to the problem and express the answer as an expected value problem\").\n",
    "\n",
    "> <div class=\"alert alert-block alert-success\">\n",
    "    Do you see the magic here? Regardless of whether we have a probability density function in the integrand and regardless of whether the domain is finite, we can always express the result as an expected value problem. It then allows us to use the sample average to approximate the solution.\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863f3898",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "[//]: # \">> For the sake of completeness:\n",
    ">>  - We recognize $p(x)$ as a density function:\n",
    " \\begin{align}\n",
    "I  = \\int_a^b f(x) p(x) dx & = E_{p}[f(x)] \\notag \\\\\n",
    "  & \\approx \\frac{1}{n}\\sum_{i=1}^n f(x_i),\\label{eq:cf1}\n",
    "\\end{align}\n",
    " where $x_i$s are drawn according to $p(x)$. \n",
    ">> \n",
    ">> \n",
    ">>  - $p(x)$ is not a weight function or we fail to recognize it as a weight function. Then we assign a uniform distribution to it. \n",
    ">> \\begin{align}\n",
    " I  = \\int_a^b f(x) p(x) dx  & = (b-a) \\int_a^b \\frac{1}{b-a} f(x) p(x) dx \\notag \\\\  \n",
    "      & =  (b-a) E_U[f(x) p(x)] \\notag \\\\   \n",
    "      & \\approx (b-a)\\left[ \\frac{1}{n}\\sum_{i=1}^n f(x_i) p(x_i) \\right], \\label{eq:cf2}\n",
    " \\end{align} \n",
    ">>  where $x_i$s are drawn from a uniform distribution $U$ in $[a,b]$.\n",
    ">>\n",
    ">> You see, you can use either \\eqref{eq:cf1} or \\eqref{eq:cf2} to compute the same integration. They differ in how $x_i$s are generated and used: In \\eqref{eq:cf1}, they are generated according to $p(x)$. In \\eqref{eq:cf2}, they are generated from a uniform distribution and then _adjusted_ in the summation through $p(x)$.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c0e6a7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### domain transformation\n",
    "\n",
    "\n",
    "\n",
    "We've seen that the key to carry out Monte Carlo integration is to use a uniform probability density function to help expressing the solution as an expected value problem. In order to use the uniform pdf, the domain of the problem has to be finite. As it turns out, it would be even better if the transformed domain is not only finite but exactly equal to $[0,1]$. If we can transform all kinds of domains to $[0,1]$, it is kind of standardizing the procedure and would make a given set of tools available to all the problems. To wit,\n",
    "\n",
    "\\begin{align}\n",
    "I  & = \\int_\\Omega f(x) dx \\label{eq:orig}\\\\\n",
    "   & = \\int_0^1 g(t) dt = (1-0) \\int_0^1 g(t) \\frac{1}{1-0} dt  = E[g(t)] \\label{eq:tran}\\\\ \n",
    "   & \\approx \\frac{1}{n}\\sum_{j=1}^n g(t_j),\\label{eq:summ}\n",
    "\\end{align}\n",
    "\n",
    "where $t_j$s are drawn from $U(0,1)$. If going from \\eqref{eq:orig} to \\eqref{eq:tran} is possible regardless of $\\Omega$, we can apply all tools that help to draw $t_j$ and calculation the sample average in \\eqref{eq:summ}.\n",
    "\n",
    "#### Remarks\n",
    "\n",
    "- What kind of sample of $(x_1, x_2, \\ldots, x_n)$ from $p(x)$ should be used?\n",
    "\n",
    "\n",
    "  - We could draw a _**(pseudo) random**_ sample of the $x_i$s, and the estimation is called the _**Monte Carlo integration (MCI)**_.\n",
    "    - The random sample can be drawn using the inverse transformation method, the rejection sampling, etc..\n",
    "    - Recall that the random numbers we generated using the computer's RNG are not truly random; they are pseudo-random.\n",
    "\n",
    "\n",
    "  - We could draw a _**quasi-random**_ sample of the $x_i$s, and the estimation is called the _**Quasi Monte Carlo integration (QMCI)**_.\n",
    "    - A class of quasi-random numbers is the low-discrepancy sequence (LDS). There are many types of LDS, among them the Halton sequence is a well-known one.\n",
    "    - QMCI has better convergence rate than the MCI.\n",
    "    \n",
    "\n",
    "  - In fact, we could also generate $(x_1, x_2, \\ldots, x_n)$ as a equally spaced grid ($x_i = i/n$) which is called the _**rectangle rule**_.\n",
    "    - The rectangle rule works good if the integration is one-dimensional, but it does not work well for multi-dimensional problems because of the correlations between the sequences. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b5ba95",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Domain Transformation\n",
    "\n",
    "\n",
    "How do we do the transformation? First, we need a transformation rule that maps $x \\in [a,b]$ to $t \\in [0,1]$, and then we apply the rule and use the changes of variables. Regarding the rule, we need to find one such that if $x=\\rho(t)$:\n",
    "\n",
    "- when $x=a$ the corresponding value is $t=0$, i.e., $a = \\rho(0)$; or, $\\rho^{-1}(a)=0$;\n",
    "\n",
    "- when $x=b$ the corresponding value is $t=1$, i.e., $b = \\rho(1)$; or, $\\rho^{-1}(b)=1$.\n",
    "\n",
    "\n",
    "If we find such a rule, $x=\\rho(t)$ and thus $\\rho^{-1}(t) = x$, we apply the change of variables on the equation:\n",
    "\n",
    "\\begin{aligned}\n",
    " f(\\rho(t)) \\rho'(t) = g(t),\n",
    "\\end{aligned}\n",
    "\n",
    "where $\\rho'(t)$ is the Jacobian. \n",
    "\n",
    "- As we will show below, if $a$ and $b$ are both finite and a suitable transformation function in this case is $x = a+(b-a)t$ to which the Jacobian is $(b-a)$.\n",
    "\n",
    "- Note that if $a$ and $b$ are finites, $I = (b-a) E_{U(a,b)}[f(X)] = E[g(t)]$. The *volume* of $(b-a)$ no longer shows using $g(t)$, and the integration is literally the sample average of the *transformed* function $g(t)$. \n",
    "\n",
    "- We sometimes see statements which equate the Monte Carlo integration to the sample average of the function. The statement might be too terse that it could be easily misunderstood. Literally speaking, the statement is correct with respect to $g(t)$ but is problematic w.r.t. $f(x)$ because the latter needs the \"volume\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3602860",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The following table provides useful rules of transformation. Note that the rules are not unique; there exist different rules to do the transformation.\n",
    "\n",
    "$$\\mathbf{x\\,\\ domain}$$ | $$\\mathbf{transformation}$$ | $$\\mathbf{t\\,\\ domain}$$ | $$\\mathbf{Jacobian}$$ \n",
    " ---     |  ---    | ---      | --- \n",
    "$$[a, b]$$            | $$x = a + (b-a)t$$ | $$[0,1]$$ | $$b - a$$\n",
    "$$[-\\infty, \\infty]$$ | $$x = \\frac{2t-1}{t-t^2}$$        | $$[0,1]$$ | $$\\frac{2t^2 - 2t+1}{(t^2 -t)^2}$$\n",
    "$$[a, \\infty]$$       | $$x = a + \\frac{t}{1-t}$$    | $$[0,1]$$ | $$\\frac{1}{(t-1)^2}$$\n",
    "$$[-\\infty, b]$$      | $$x = b + \\frac{t-1}{t}$$    | $$[0,1]$$ | $$\\frac{1}{t^2}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67c5df8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here, we give a more complete argument why we prefer transforming the domain to $[0,1]$:\n",
    "\n",
    "- We know rules of transforming $[a,b]$ to $[0,1]$ for $a$ and $b$ ranging from $-\\infty$ to $\\infty$, so this part is not difficult.\n",
    "\n",
    "- After the conversion, the random numbers are all drawn from $[0,1]$ instead of $[a,b]$. Thus, the sample from $[0,1]$ could be repeatedly used for different problems.\n",
    "\n",
    "- Any multidimensional function with bounds on each variable can be transformed into the unit n-dimensional hypercube, $[0, 1]^d$.\n",
    "\n",
    "- Because of the above, it's easy to write a computer program to automate the process, from domain and function transformation to random number sampling and to computing the final result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73acf9ae",
   "metadata": {},
   "source": [
    "### review summary\n",
    "\n",
    "> **We start with a general setup in which there is a probability density function $p(x)$ in the integrand and the density's support is equal to $\\Omega$. We introduce the notation of $E_p[f(x)]$ and $E[f(x)]$.**\n",
    ">> \n",
    ">> - Most general:\n",
    "\\begin{align}\n",
    "I  = \\int_\\Omega f(x) p(x) dx = E_p[f(x)],\\quad  \\mbox{$x$ follows $p(x)$},\n",
    "\\end{align}\n",
    ">> \n",
    ">>\n",
    ">> - If $p(x)$ is the pdf of $U(0,1)$ and $\\Omega = [0,1]$:\n",
    "\\begin{align}\n",
    "I  = \\int_0^1 f(x) p(x) dx = \\int_0^1 f(x) dx =E[f(x)],\\quad  \\mbox{$x$ follows $U(0,1)$},\\label{eq:general2}\n",
    "\\end{align}\n",
    "> \n",
    "> \n",
    "> **We then talk about the common problem where there is no weight function $p(x)$ in the integrand (i.e., $p(x)=1$ regardless of $\\Omega$). The trick is to transform the problem from the domain $\\Omega$ to the domain $\\tilde{\\Omega} = [0,1]$. Then, something along the line of \\eqref{eq:general2} can be done. The difficult part is in carrying out the transformation.**\n",
    "> \n",
    "> \n",
    ">> A special case where $p(x)=1$ (i.e., no weight function) regardless of $\\Omega$:\n",
    ">> \n",
    ">> - If $\\Omega$ is finite:\n",
    "\\begin{aligned}\n",
    "I  = \\int_a^b f(x) dx = (b-a)E_U[f(x)],\\quad  \\mbox{$x$ follows $U(a,b)$}.\n",
    "\\end{aligned}\n",
    ">> \n",
    ">>\n",
    ">> - If $\\Omega$ is not only finite but also $[0,1]$:\n",
    "\\begin{aligned}\n",
    "I  = \\int_0^1 f(x) dx = E[f(x)],\\quad  \\mbox{$x$ follows $U(0,1)$}.\n",
    "\\end{aligned}\n",
    ">> \n",
    ">>\n",
    ">> - If $\\Omega$ is not finite:\n",
    ">>   - First, we map the problem's domain from $x$ (infinite domain) to $t$ which has a finite domain $\\tilde{\\Omega} = [a,b]$, preferably $\\tilde{\\Omega} = [0,1]$, \n",
    ">>   - Then,\n",
    "\\begin{aligned}\n",
    "I  = \\int_0^1 g(t) dx = E[g(t)],\\quad  \\mbox{$t$ follows $U(0,1)$}.\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5812457",
   "metadata": {},
   "source": [
    "## Integration using Importance Sampling\n",
    "\n",
    "### Motivation\n",
    "\n",
    "\n",
    "We show in previous sections that we can transform an integration problem into a problem of computing the expected value of the function. It takes three basic forms:\n",
    "    \n",
    " \\begin{align}\n",
    "   I & = \\int_\\Omega f(x) p(x) dx = E_p[f(x)],\\quad  \\mbox{$x$ follows $p(x)$}, \\label{eq:case1}\\\\\n",
    "   I & = \\int_a^b f(x)  dx = (b-a) E_U[f(x)], \\quad  \\mbox{$x$ follows $U(a,b)$}, \\label{eq:case2}\\\\\n",
    "   I & = \\int_\\Omega h(x) dx = \\int_0^1 g(t) dt = E[g(t)],\\quad  \\mbox{$t$ follows $U(0,1)$}. \\label{eq:case3}\n",
    "   \\end{align}\n",
    "\n",
    "\n",
    "Though the approach is very useful, it may not be the most efficient method for some problems. Let's consider the following scenarios.\n",
    "\n",
    "\n",
    "- In regard with \\eqref{eq:case2}: The uniform sampling in $[a,b]$ may not be efficient if the value of $f(x)$ mostly comes from a particular region in the support. It would be more efficient if we could sample more heavily in that region and lightly in other region. Instead, the uniform sampling from $U(a,b)$ means that the occurrence of $x$ is assumed to be equally likely for all values in $[a,b]$. \n",
    "\n",
    "\n",
    "- In regard with \\eqref{eq:case3}: The transform rule used to map $x\\in \\Omega$ into $t \\in [0,1]$ may be quite nonlinear, which would impact the efficiency of the sampling. Thus, if we have a problem in the form of \\eqref{eq:case1}, sometimes we don't want to go the route of \\eqref{eq:case3) (by making $h(x)\\equiv f(x)p(x)$).\n",
    "\n",
    "\n",
    "- However, even with \\eqref{eq:case1}, sometimes $p(x)$ is too difficult to sample from.\n",
    "\n",
    "\n",
    "The above problems may be circumvented using the importance sampling method.\n",
    "\n",
    "\n",
    "### How does it work\n",
    "\n",
    "#### example 1\n",
    "\n",
    "Consider the issue we mentioned in regard with \\eqref{eq:case2}. Suppose we know $f(x)$ is the largest around $x=3$, we could use a _**normalized**_ normal distribution (or, a truncated normal distribution) with the mean equal to $3$ as the density function, which will draw more samples from $x$ around $3$ and less samples elsewhere. Call this density function $q(x)$ where  $\\int_a^b q(x) dx =1$. We then have\n",
    "\n",
    "\\begin{aligned}\n",
    "I = \\int_a^b f(x) dx = \\int_a^b \\frac{f(x)}{q(x)} q(x) dx = E_{q}[h(x)] \\approx \\frac{1}{n}\\sum_{j=1}^n h(x_j),\n",
    "\\end{aligned}\n",
    "\n",
    "where $h(x) = [f(x)/q(x)]$ and $x_j$s are drawn according to $q(x)$. Here, $q(x)$ is often called a _**proposal distribution**_ or _**sampling distribution**_.\n",
    "\n",
    "The above is an example of importance sampling. A good choice of $q(x)$ provides better sampling, which results in reduced variance and faster convergence. Therefore, importance sampling is often referred to as a variance reduction method.\n",
    "\n",
    "#### example 2\n",
    "\n",
    "Let's consider the issue we mentioned in regard with \\eqref{eq:case1} where $p(x)$ is difficult to sample from. Suppose $q2(x)$ is a probability density function ($\\int_\\Omega q2(x)dx=1$) which is easier to sample. We could transform the problem into one that uses $q2(x)$, not $p(x)$, as the probability density.\n",
    "\n",
    "\n",
    "\\begin{aligned}\n",
    "I = \\int_\\Omega f(x)p(x) dx = \\int_\\Omega \\left[\\frac{f(x)p(x)}{q2(x)}\\right] q2(x) dx = E_{q2}[h(X)],\n",
    "\\end{aligned}\n",
    "\n",
    "where $h(x) = [f(x)p(x)/q2(x)]$. To approximate the integral,  \n",
    "\n",
    "\\begin{aligned}\n",
    "I = E_{q2}[h(X)] \\approx \\frac{1}{n} \\sum_{j=1}^n h(x_j),\n",
    "\\end{aligned}\n",
    "\n",
    "where the $x_j$s are sampled based on $q2(x)$ (rather than $p(x)$). \n",
    "\n",
    "\n",
    "### Discussion\n",
    "\n",
    "\n",
    "An interesting issue to note is that the sampling distribution (i.e., $q(x)$ and $q2(x)$) is not necessarily the exact true distribution of $x$ and is in fact likely to be a biased distribution to $x$. So, would using it bias the estimate? No, it wouldn't. It is because the sampling is weighted to correct for the use of the biased distribution, and the correction ensures that the estimator is unbiased. The weight is given by $p(x)/q2(x)$ which is called the _**likelihood ratio**_. \n",
    "\n",
    "\n",
    "Choosing good proposal distributions of $q(x)$ and $q2(x)$ are vital. It gives a simpler expression and efficient sampling. Some wisdoms from the literature (using our last example to illustrate):\n",
    "- select a $q2(x)$ which comes from the same family of $p(x)$ so that they have similar shapes;\n",
    "- $q2(x)$ should have thicker tails than $p(x)$, otherwise $h(x)=f(x)p(x)/q2(x)$ may get too large and become unbounded upward;\n",
    "- $q2(x)$ should be easy to do sampling from.\n",
    "\n",
    "\n",
    "#### Other Remarks\n",
    "\n",
    "- GHK simulator is a kind of importance sampling where it uses a truncated normal as the probability function. See also `(Gates 2006 SJ) A Mata Geweke-Hajivassiliou-Keane Multivariate Normal Simulator.pdf`,  `(Benz Bretz 2009) Computation of Multivariate Normal and t Probabilities.pdf`.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "3dd573130a0147618f53a10f137d62a9",
   "lastKernelId": "531ea0f0-0ac5-4cc3-9027-ec73fe946255"
  },
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "238.08px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML2022Spring - HW11.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5cFq_TgWlQ_"
      },
      "source": [
        "# Homework 11 - Transfer Learning (Domain Adversarial Training)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNiZCGrIYKdR"
      },
      "source": [
        "# Readme\n",
        "\n",
        "In homework 11, you will need to implement Domain Adversarial Training in Transfer Learning. As shown in the bottom left part of the figure.\n",
        "\n",
        "<img src=\"https://i.imgur.com/iMVIxCH.png\" width=\"500px\">\n",
        "\n",
        "> \n",
        "\n",
        "## Scenario and Why Domain Adversarial Training\n",
        "Now we have labeled source data and unlabeled target data, where source data might be relavent to the target data. We now want to train a model with source data only and test it on target data.\n",
        "\n",
        "What problem might occur if we do so? After we have learned Anomaly Detection, we now know that if we test the model with an abnormal data that have never appeared in source data, our trained model is likely to result in poor performance since it is not familiar with the abnormal data.\n",
        "\n",
        "For example, we have a model that contains Feature Extractor and Classifier:\n",
        "<img src=\"https://i.imgur.com/IL0PxCY.png\" width=\"500px\">\n",
        "\n",
        "When the model is trained with source data, the feature extractor \n",
        "will extract meaningful features since it is familiar with the distribution of it.It could be seen in the following figure that the blue dots, which is the distribution of source data, has already been clustered into different clusters. Therefore, the Classifier can predict the label based on these clusters.\n",
        "\n",
        "However, when test on the target data, the Feature Extractor will not be able to extract meaningful features that follow the distribution of the source feature distribution, which result in the classifier learned for the source domain will not be able to apply to the target domain.\n",
        "\n",
        "\n",
        "## Domain Adversarial Training of Nerural Networks (DaNN)\n",
        "\n",
        "Based on the above problems, DaNN approaches build mappings between the source (training-time) and the target (test-time) domains, so that the classifier learned for the source domain can also be applied to the target domain, when composed with the learned mapping between domains.\n",
        "\n",
        "<img src=\"https://i.imgur.com/vrOE5a6.png\" width=\"500px\">\n",
        "\n",
        "In DaNN, the authors added a Domain Classifier, which is a deep discriminatively-trained classifeir in the training framework to distinguish the data from different domain by the features extracted by the feature extractor. As the training progresses, the approach promotes a domain classifier that discriminates between the source and the target domains and a feature extractor that can extractor features that are discriminative for the main learning task on the source domain and indiscriminate with respect to the shift between the domains. \n",
        "\n",
        "\n",
        "The feature extractor are likely to outperform the domain classifier as its input are generated by the feature extractor and that the task of domain classification and label classification are not conflict.\n",
        "\n",
        "This method leads to the emergence of features that are domain-invariant and on the same feature distribution."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "GPU_name = torch.cuda.get_device_name()\n",
        "print(\"Your GPU is {}!\".format(GPU_name))"
      ],
      "metadata": {
        "id": "2ZJIIwMXWSCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "m6XD_QgsWebY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-qnUkspmap3"
      },
      "source": [
        "# Data Introduce\n",
        "\n",
        "Our task contains source data: real photos, and target data: hand-drawn graffiti.\n",
        "\n",
        "We are going to train the model with the photos and the labels, and try to predict what the labels are for hand-drawn graffiti.\n",
        "\n",
        "The data could be downloaded [here](https://github.com/redxouls/ml2020spring-hw11-dataset/releases/download/v1.0.0/real_or_drawing.zip). The code below is for data downloading and visualization.\n",
        "\n",
        "Note that: **The source and target data are all balanced data, you can make use of this information.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DF-i0sVlnUbq"
      },
      "source": [
        "# Download dataset\n",
        "!wget \"https://github.com/redxouls/ml2020spring-hw11-dataset/releases/download/v1.0.0/real_or_drawing.zip\" -O real_or_drawing.zip\n",
        "\n",
        "# Download from mirrored dataset link\n",
        "# !wget \"https://github.com/redxouls/ml2020spring-hw11-dataset/releases/download/v1.0.1/real_or_drawing.zip\" -O real_or_drawing.zip\n",
        "# !wget \"https://github.com/redxouls/ml2020spring-hw11-dataset/releases/download/v1.0.2/real_or_drawing.zip\" -O real_or_drawing.zip\n",
        "\n",
        "# Unzip the files\n",
        "!unzip real_or_drawing.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_uO-ZSDoR6i"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def no_axis_show(img, title='', cmap=None):\n",
        "  # imshow, and set the interpolation mode to be \"nearest\"ã€‚\n",
        "  fig = plt.imshow(img, interpolation='nearest', cmap=cmap)\n",
        "  # do not show the axes in the images.\n",
        "  fig.axes.get_xaxis().set_visible(False)\n",
        "  fig.axes.get_yaxis().set_visible(False)\n",
        "  plt.title(title)\n",
        "\n",
        "titles = ['horse', 'bed', 'clock', 'apple', 'cat', 'plane', 'television', 'dog', 'dolphin', 'spider']\n",
        "plt.figure(figsize=(18, 18))\n",
        "for i in range(10):\n",
        "  plt.subplot(1, 10, i+1)\n",
        "  fig = no_axis_show(plt.imread(f'real_or_drawing/train_data/{i}/{500*i}.bmp'), title=titles[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eMs7DbVt4Ee"
      },
      "source": [
        "plt.figure(figsize=(18, 18))\n",
        "for i in range(10):\n",
        "  plt.subplot(1, 10, i+1)\n",
        "  fig = no_axis_show(plt.imread(f'real_or_drawing/test_data/0/' + str(i).rjust(5, '0') + '.bmp'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moXQw9To5TqZ"
      },
      "source": [
        "# Special Domain Knowledge\n",
        "\n",
        "When we graffiti, we usually draw the outline only, therefore we can perform edge detection processing on the source data to make it more similar to the target data.\n",
        "\n",
        "\n",
        "## Canny Edge Detection\n",
        "The implementation of Canny Edge Detection is as follow.\n",
        "The algorithm will not be describe thoroughly here.  If you are interested, please refer to the wiki or [here](https://medium.com/@pomelyu5199/canny-edge-detector-%E5%AF%A6%E4%BD%9C-opencv-f7d1a0a57d19).\n",
        "\n",
        "We only need two parameters to implement Canny Edge Detection with CV2:  `low_threshold` and `high_threshold`.\n",
        "\n",
        "```cv2.Canny(image, low_threshold, high_threshold)```\n",
        "\n",
        "Simply put, when the edge value exceeds the high_threshold, we determine it as an edge. If the edge value is only above low_threshold, we will then determine whether it is an edge or not.\n",
        "\n",
        "Let's implement it on the source data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mn2MkDLV7E2-"
      },
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "titles = ['horse', 'bed', 'clock', 'apple', 'cat', 'plane', 'television', 'dog', 'dolphin', 'spider']\n",
        "plt.figure(figsize=(18, 18))\n",
        "\n",
        "original_img = plt.imread(f'real_or_drawing/train_data/0/0.bmp')\n",
        "plt.subplot(1, 5, 1)\n",
        "no_axis_show(original_img, title='original')\n",
        "\n",
        "gray_img = cv2.cvtColor(original_img, cv2.COLOR_RGB2GRAY)\n",
        "plt.subplot(1, 5, 2)\n",
        "no_axis_show(gray_img, title='gray scale', cmap='gray')\n",
        "\n",
        "gray_img = cv2.cvtColor(original_img, cv2.COLOR_RGB2GRAY)\n",
        "plt.subplot(1, 5, 2)\n",
        "no_axis_show(gray_img, title='gray scale', cmap='gray')\n",
        "\n",
        "canny_50100 = cv2.Canny(gray_img, 50, 100)\n",
        "plt.subplot(1, 5, 3)\n",
        "no_axis_show(canny_50100, title='Canny(50, 100)', cmap='gray')\n",
        "\n",
        "canny_150200 = cv2.Canny(gray_img, 150, 200)\n",
        "plt.subplot(1, 5, 4)\n",
        "no_axis_show(canny_150200, title='Canny(150, 200)', cmap='gray')\n",
        "\n",
        "canny_250300 = cv2.Canny(gray_img, 250, 300)\n",
        "plt.subplot(1, 5, 5)\n",
        "no_axis_show(canny_250300, title='Canny(250, 300)', cmap='gray')\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8THSdt_hmwYh"
      },
      "source": [
        "# Data Process\n",
        " \n",
        " \n",
        "The data is suitible for `torchvision.ImageFolder`. You can create a dataset with `torchvision.ImageFolder`. Details for image augmentation please refer to the comments in the following codes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZHIBGknmi8Z"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Function\n",
        " \n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        " \n",
        "source_transform = transforms.Compose([\n",
        "    # Turn RGB to grayscale. (Bacause Canny do not support RGB images.)\n",
        "    transforms.Grayscale(),\n",
        "    # cv2 do not support skimage.Image, so we transform it to np.array, \n",
        "    # and then adopt cv2.Canny algorithm.\n",
        "    transforms.Lambda(lambda x: cv2.Canny(np.array(x), 170, 300)),\n",
        "    # Transform np.array back to the skimage.Image.\n",
        "    transforms.ToPILImage(),\n",
        "    # 50% Horizontal Flip. (For Augmentation)\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    # Rotate +- 15 degrees. (For Augmentation), and filled with zero \n",
        "    # if there's empty pixel after rotation.\n",
        "    transforms.RandomRotation(15, fill=(0,)),\n",
        "    # Transform to tensor for model inputs.\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "target_transform = transforms.Compose([\n",
        "    # Turn RGB to grayscale.\n",
        "    transforms.Grayscale(),\n",
        "    # Resize: size of source data is 32x32, thus we need to \n",
        "    #  enlarge the size of target data from 28x28 to 32x32ã€‚\n",
        "    transforms.Resize((32, 32)),\n",
        "    # 50% Horizontal Flip. (For Augmentation)\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    # Rotate +- 15 degrees. (For Augmentation), and filled with zero \n",
        "    # if there's empty pixel after rotation.\n",
        "    transforms.RandomRotation(15, fill=(0,)),\n",
        "    # Transform to tensor for model inputs.\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        " \n",
        "source_dataset = ImageFolder('real_or_drawing/train_data', transform=source_transform)\n",
        "target_dataset = ImageFolder('real_or_drawing/test_data', transform=target_transform)\n",
        " \n",
        "source_dataloader = DataLoader(source_dataset, batch_size=32, shuffle=True)\n",
        "target_dataloader = DataLoader(target_dataset, batch_size=32, shuffle=True)\n",
        "test_dataloader = DataLoader(target_dataset, batch_size=128, shuffle=False)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set seed"
      ],
      "metadata": {
        "id": "DjkPxBmxdhfY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "def same_seeds(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "same_seeds(890727)"
      ],
      "metadata": {
        "id": "mO0WqTCDdgjN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdwDEMrOycs5"
      },
      "source": [
        "# Model\n",
        "\n",
        "Feature Extractor: Classic VGG-like architecture\n",
        "\n",
        "Label Predictor / Domain Classifier: Linear models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uw2eP09z-pD"
      },
      "source": [
        "class FeatureExtractor(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, 3, 1, 1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(64, 128, 3, 1, 1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(128, 256, 3, 1, 1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(256, 256, 3, 1, 1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(256, 512, 3, 1, 1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv(x).squeeze()\n",
        "        return x\n",
        "\n",
        "class LabelPredictor(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(LabelPredictor, self).__init__()\n",
        "\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Linear(512, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(512, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(512, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, h):\n",
        "        c = self.layer(h)\n",
        "        return c\n",
        "\n",
        "class DomainClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(DomainClassifier, self).__init__()\n",
        "\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Linear(512, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(512, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(512, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(512, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(512, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, h):\n",
        "        y = self.layer(h)\n",
        "        return y"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxdBIPhF0Icb"
      },
      "source": [
        "# Pre-processing\n",
        "\n",
        "Here we use Adam as our optimizor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrxKelBy0PJ7"
      },
      "source": [
        "feature_extractor = FeatureExtractor().cuda()\n",
        "label_predictor = LabelPredictor().cuda()\n",
        "domain_classifier = DomainClassifier().cuda()\n",
        "\n",
        "class_criterion = nn.CrossEntropyLoss()\n",
        "domain_criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "optimizer_F = optim.Adam(feature_extractor.parameters(), lr=13e-4)\n",
        "optimizer_C = optim.Adam(label_predictor.parameters(), lr=13e-4)\n",
        "optimizer_D = optim.Adam(domain_classifier.parameters(), lr=13e-4)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuAE4cqJ0itR"
      },
      "source": [
        "# Start Training\n",
        "\n",
        "\n",
        "## DaNN Implementation\n",
        "\n",
        "In the original paper, Gradient Reversal Layer is used.\n",
        "Feature Extractor, Label Predictor, and Domain Classifier are all trained at the same time. In this code, we train Domain Classifier first, and then train our Feature Extractor (same concept as Generator and Discriminator training process in GAN).\n",
        "\n",
        "## Reminder\n",
        "* Lambda, which controls the domain adversarial loss, is adaptive in the original paper. You can refer to [the original work](https://arxiv.org/pdf/1505.07818.pdf) . Here lambda is set to 0.1.\n",
        "* We do not have the label to target data, you can only evaluate your model by uploading your result to kaggle.:)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRAFFKvX0p9y"
      },
      "source": [
        "def train_epoch(source_dataloader, target_dataloader, lamb=0.1):\n",
        "    '''\n",
        "      Args:\n",
        "        source_dataloader: source dataçš„dataloader\n",
        "        target_dataloader: target dataçš„dataloader\n",
        "        lamb: control the balance of domain adaptatoin and classification.\n",
        "    '''\n",
        "\n",
        "    # D loss: Domain Classifierçš„loss\n",
        "    # F loss: Feature Extrator & Label Predictorçš„loss\n",
        "    running_D_loss, running_F_loss = 0.0, 0.0\n",
        "    total_hit, total_num = 0.0, 0.0\n",
        "\n",
        "    for i, ((source_data, source_label), (target_data, _)) in enumerate(zip(source_dataloader, target_dataloader)):\n",
        "\n",
        "        source_data = source_data.cuda()\n",
        "        source_label = source_label.cuda()\n",
        "        target_data = target_data.cuda()\n",
        "        \n",
        "        # Mixed the source data and target data, or it'll mislead the running params\n",
        "        #   of batch_norm. (runnning mean/var of soucre and target data are different.)\n",
        "        mixed_data = torch.cat([source_data, target_data], dim=0)\n",
        "        domain_label = torch.zeros([source_data.shape[0] + target_data.shape[0], 1]).cuda()\n",
        "        # set domain label of source data to be 1.\n",
        "        domain_label[:source_data.shape[0]] = 1\n",
        "\n",
        "        # Step 1 : train domain classifier\n",
        "        feature = feature_extractor(mixed_data)\n",
        "        # We don't need to train feature extractor in step 1.\n",
        "        # Thus we detach the feature neuron to avoid backpropgation.\n",
        "        domain_logits = domain_classifier(feature.detach())\n",
        "        loss = domain_criterion(domain_logits, domain_label)\n",
        "        running_D_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # Step 2 : train feature extractor and label classifier\n",
        "        class_logits = label_predictor(feature[:source_data.shape[0]])\n",
        "        domain_logits = domain_classifier(feature)\n",
        "        # loss = cross entropy of classification - lamb * domain binary cross entropy.\n",
        "        #  The reason why using subtraction is similar to generator loss in disciminator of GAN\n",
        "        loss = class_criterion(class_logits, source_label) - lamb * domain_criterion(domain_logits, domain_label)\n",
        "        running_F_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer_F.step()\n",
        "        optimizer_C.step()\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "        optimizer_F.zero_grad()\n",
        "        optimizer_C.zero_grad()\n",
        "\n",
        "        total_hit += torch.sum(torch.argmax(class_logits, dim=1) == source_label).item()\n",
        "        total_num += source_data.shape[0]\n",
        "        print(i, end='\\r')\n",
        "\n",
        "    return running_D_loss / (i+1), running_F_loss / (i+1), total_hit / total_num"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "max_lamb = 3\n",
        "epoch_num = 2500\n",
        "\n",
        "for epoch in range(epoch_num):\n",
        "    lamb = max_lamb / (1 + 2 * math.exp(-10 * (epoch / epoch_num))) - 1\n",
        "  \n",
        "    train_D_loss, train_F_loss, train_acc = train_epoch(source_dataloader, target_dataloader, lamb=lamb)\n",
        "\n",
        "    if (epoch + 1) % 500 == 0:    \n",
        "        torch.save(feature_extractor.state_dict(), f'./gdrive/MyDrive/ML2022/ML2022_hw11/extractor_model{(epoch + 1) // 500}.bin')\n",
        "        torch.save(label_predictor.state_dict(), f'./gdrive/MyDrive/ML2022/ML2022_hw11/predictor_model{(epoch + 1) // 500}.bin')\n",
        "\n",
        "    print('epoch {:>3d}: train D loss: {:6.4f}, train F loss: {:6.4f}, acc {:6.4f}'.format(epoch, train_D_loss, train_F_loss, train_acc))\n"
      ],
      "metadata": {
        "id": "2Fr4_7d6mcQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8_-0iSSje4w"
      },
      "source": [
        "# Inference\n",
        "\n",
        "We use pandas to generate our csv file.\n",
        "\n",
        "BTW, the performance of the model trained for 200 epoches might be unstable. You can train for more epoches for a more stable performance."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extractors = []\n",
        "predictors = []\n",
        "for i in range(1, 6):\n",
        "    feature_extractor.load_state_dict(torch.load(f'./gdrive/MyDrive/ML2022/ML2022_hw11/extractor_model{i}.bin', map_location='cuda'))\n",
        "    label_predictor.load_state_dict(torch.load(f'./gdrive/MyDrive/ML2022/ML2022_hw11/predictor_model{i}.bin', map_location='cuda'))\n",
        "    extractors.append(feature_extractor.eval())\n",
        "    predictors.append(label_predictor.eval())"
      ],
      "metadata": {
        "id": "c13jf3CQqoiQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wly5AgH2jePv"
      },
      "source": [
        "result = []\n",
        "\n",
        "for i, (test_data, _) in enumerate(test_dataloader):\n",
        "    for j in range(len(extractors)):\n",
        "        test_data = test_data.cuda()\n",
        "\n",
        "        if j == 0:\n",
        "            class_logits = predictors[j](extractors[j](test_data))\n",
        "        else:\n",
        "            class_logits += predictors[j](extractors[j](test_data))\n",
        "            \n",
        "    class_logits /= len(extractors)\n",
        "    x = torch.argmax(class_logits, dim=1).cpu().detach().numpy()\n",
        "    result.append(x)\n",
        "\n",
        "import pandas as pd\n",
        "result = np.concatenate(result)\n",
        "\n",
        "# Generate your submission\n",
        "df = pd.DataFrame({'id': np.arange(0,len(result)), 'label': result})\n",
        "df.to_csv('./gdrive/MyDrive/ML2022/ML2022_hw11/DaNN_submission.csv',index=False)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization\n",
        "We use t-SNE plot to observe the distribution of extracted features."
      ],
      "metadata": {
        "id": "_X8v0wojx1jD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpheoH_rvFbO"
      },
      "source": [
        "# Training Statistics\n",
        "\n",
        "- Number of parameters:\n",
        "  - Feature Extractor: 2, 142, 336\n",
        "  - Label Predictor: 530, 442\n",
        "  - Domain Classifier: 1, 055, 233\n",
        "\n",
        "- Simple\n",
        " - Training time on colab: ~ 1 hr\n",
        "- Medium\n",
        " - Training time on colab: 2 ~ 4 hr\n",
        "- Strong\n",
        " - Training time on colab: 5 ~ 6 hrs\n",
        "- Boss\n",
        " - **Unmeasurable**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYO8InxavGsy"
      },
      "source": [
        "# Learning Curve (Strong Baseline)\n",
        "* This method is slightly different from colab.\n",
        "\n",
        "![Loss Curve](https://i.imgur.com/vIujQyo.png)\n",
        "\n",
        "# Accuracy Curve (Strong Baseline)\n",
        "* Note that you cannot access testing accuracy. But this plot tells you that even though the model overfits the training data, the testing accuracy is still improving, and that's why you need to train more epochs.\n",
        "\n",
        "![Acc Curve](https://i.imgur.com/4W1otXG.png)\n",
        "\n"
      ]
    }
  ]
}